{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Data Science: Lab 7 - Similarity\n",
    " \n",
    "Below, we create a list of docs - which are the course descriptions for the core modules on the Sussex MSc. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "\n",
    "docs.append(\"The module teaches the computer science aspects of data science. A particular focus is on how data are represented and manipulated to achieve good performance on large data sets (> 10 GBytes) where standard techniques may no longer apply. In lectures, students will learn about data structures, algorithms, and systems, including distributed computing, databases (relational and non-relational), parallel computing, and cloud computing. In laboratory sessions, students will develop their Python programming skills; work with a variety of data sets including large data sets from real world applications; and investigate the impact on run-time of their algorithmic choices.\")\n",
    "\n",
    "docs.append(\"This module introduces you to the mathematical and statistical techniques used to analyse data. The module is fairly rigorous, and is aimed at students who have, or anticipate having, research data to analyse in a thorough and unbiased way.  Topics include: probability distributions; error propagation; maximum likelihood method and linear least squares fitting; chi-squared testing; subjective probability and Bayes' theorem; monte Carlo techniques; and non-linear least squares fitting.\")\n",
    "\n",
    "docs.append(\"In this module, you use literature to study the background to a problem in Data Science in your respective stream.  You choose your individual supervisor and devise a strategy by which this problem can be studied - giving details of techniques and resources that you will use to address the problem.  This research proposal forms the basis of the Data Science Dissertation that you will write in summer term.\")\n",
    "\n",
    "docs.append(\"For this module, you carry out independent study and research under the guide of a supervisor on a designated topic. You then complete a report on the subject over the summer.\")\n",
    "\n",
    "docs.append(\"In this module, you explore advanced techniques in machine learning. You use a systematic treatment, based on the following three key ingredients: tasks, models, features.  As part of the module, you are introduced to both regression and classification, and your studies emphasise concepts such as model performance, learnability and computational complexity.  You learn techniques including: probabilistic and non-probabilistic classification and regression methods, reinforcement learning approaches including the non-linear variants using kernel methods.  You are also introduced to techniques for pre-processing the data (including PCA). You will then need to be able to implement, develop and deploy these techniques to real-world problems.  In order to take this module, you need to have already taken the 'Mathematics & Computational Methods for Complex Systems' module (817G5), or have taken an equivalent mathematical module or have equivalent prior experience.\")\n",
    "\n",
    "docs.append(\"Your studies in this module include a series of seminars covering several topics - including national laws on data and ethical implications.  In addition, there are seminars by Data-Science-oriented companies.  You are expected to write a final dissertation of 3000 words, on a topic of your choice falling within the scope of the module.\")\n",
    "\n",
    "docs.append(\"This module will provide students with the practical tools and techniques required to build, analyse and interpret 'big data' datasets. It will cover all aspects of the Data Science process including collection, munging or wrangling, cleaning, exploratory data analysis, visualization, statistical inference and model building and implications for applications in the real world. During the module, they will be taught how to scrape data from the Internet, develop and test hypotheses, use principal component analysis (PCA) to reduce dimensionality, prepare actionable plans and present their findings. In the laboratory, students will develop their Python programming skills and be introduced to a number of fundamental standard Python libraries/toolkits for Data Scientists including NumPy, SciPy, PANDAS and SCIKIT-Learn. In these sessions and their coursework, students will work with real-world datasets and apply the techniques covered in lectures to that data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the nltk library to tokenise these.  Tokenization turns them into lists of words (and deals with punctuation better than if we just split the documents according to whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "bags=[]\n",
    "for doc in docs:\n",
    "    bags.append(word_tokenize(doc))\n",
    "    \n",
    "print(bags[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this has an error, you probably need to run the nltk downloader on your machine to get the necessary resources. If this happens, then run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write a function to turn a list of tokens into a dictionary of counts.  Apply this function to each of the lists in bags to create a representation for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "Write a function to compute the Jaccard similarity between two of your bags (represented as dictionaries). (You can find this in the lecture notes.) If you test it on the first two documents you should get a similarity of 0.132 (to 3SF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Compute all pairs Jaccard similarities for the 7 documents.  Which pair of documents are most similar?  Which pair are least similar?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will turn the dictionary representation of documents into a characteristic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(list_of_dicts):\n",
    "    #first of all make a list of all of the features that occur in any document - these will be the dimensions of the matrix\n",
    "    allfeatures={}    \n",
    "    for docdict in list_of_dicts:\n",
    "        for feat in docdict.keys():\n",
    "            allfeatures[feat]=1\n",
    "    \n",
    "    dimensions=list(allfeatures.keys())\n",
    "    #don't strictly need to sort it - but it is good practise to make sure it is reproducible\n",
    "    sorted(dimensions)\n",
    "    \n",
    "    matrix=[]\n",
    "    #each row in the matrix will be one of the dimensions\n",
    "    for dimension in dimensions:\n",
    "        row=[]\n",
    "        #look up the appropriate value for each document\n",
    "        for docdict in list_of_dicts:\n",
    "            row.append(docdict.get(dimension,0)) #this will append the document's value if present, 0 otherwise\n",
    "        matrix.append(row)\n",
    "        \n",
    "        \n",
    "    return matrix\n",
    "\n",
    "## it might be useful to be able to transpose a matrix so we can compare documents\n",
    "def transpose(matrix):\n",
    "    transposed=[]\n",
    "    for i in range(0,len(matrix[0])):\n",
    "        transposed.append([row[i] for row in matrix])\n",
    "        \n",
    "    return transposed\n",
    "\n",
    "#call make_matrix on whatever variable holds your list of dictionaries representation of your documents\n",
    "amatrix=make_matrix(docdicts)\n",
    "print(amatrix)\n",
    "print(transpose(amatrix))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "Write a function to compute the cosine similarity of two documents. (You can find this in the lecture notes.)  Test it on the first 2 documents - you should get 0.471(to 3SF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "Compute the all pairs cosine similarities for the 6 documents.  Which is the most similar pair of documents?  Which is the least similar pair of documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "What is the computational complexity of the Jaccard algorithm from Exercise 2? Test this empirically. What is the constant? You may want to make some longer documents - you could do this by concatenating different numbers of bags that were created for the original documents and / or sampling from a very large bag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "What is the computational complexity of the cosine algorithm from Exercise 4?  Test this empirically.  What is the constant?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
