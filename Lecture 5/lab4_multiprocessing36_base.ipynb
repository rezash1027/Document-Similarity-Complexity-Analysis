{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4-multiprocessing36_base.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjYnZ7yjzX6A"
      },
      "source": [
        "# Week 4: Multiprocessing in Python\n",
        "In this exercise, we are exploring the multiprocessing capabilities of Python to support the concepts of Processes, Process Lifecycle, Concurrent Execution and Synchronization.\n",
        "\n",
        "## Working on Windows\n",
        "Unfortunately, interactive Python doesn't play well with multiprocessing on Windows.  The problem is that the module namespace isn't inherited from the parent, so the function names that are the targets for the new processes aren't available.  We are therefore going to execute this exercise on colab, a Google hosted Jupyter environment, available at https://colab.research.google.com.  Alternatively, this is probably the time to understand how to program directly in Python, and to use editors such as Notepad++ or emacs to create python files directly, and run them from the command line. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn9sLXyYzX6B"
      },
      "source": [
        "## Simple multiprocessing\n",
        "The multiprocessing package supports the spawning of separate processes running your Python code. In the following code, we create the given number of processes. Note that these are operating system level processes with their own threads of control and address spaces. There is magic to allow some shared state (investigate the operating system \"fork\" to see how)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzZAy-IdzX6D"
      },
      "source": [
        "import multiprocessing\n",
        "\n",
        "def worker(msg):\n",
        "    print(multiprocessing.current_process(), \" says \",msg)\n",
        "    \n",
        "def spawnWorkers(num,msg):\n",
        "    for w in range(num):\n",
        "        p = multiprocessing.Process(target=worker,args=(msg,))\n",
        "        p.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VEsNqvkzX6G"
      },
      "source": [
        "spawnWorkers(4,\"Hello World\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMmQPronzX6J"
      },
      "source": [
        "The processes above are simple processes that execute quickly and then exit. If we have longer lived process, it is good practice to have the parent process (the process spawning the children) wait for the children to terminate - a call to the join() method. So in general, we use the following idiom to manage individual processes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP2hdUQpzX6K"
      },
      "source": [
        "def waitForWorkers(num,msg):\n",
        "    processes = []\n",
        "    for w in range(num):\n",
        "        p = multiprocessing.Process(target=worker,args=(msg,))\n",
        "        processes.append(p)\n",
        "        p.start()\n",
        "    for p in processes:\n",
        "        p.join()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znt7K97IzX6N"
      },
      "source": [
        "waitForWorkers(4,\"hello world\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBVe3Wk0zX6Q"
      },
      "source": [
        "Let's create a program to investigate how processes are mapped to cores. For this, we're going to create a worker function that executes a lot of instructions (CPU-Bound), and we're going to time how long it takes. If each process is mapped to a different core, then each process should take the approximately the same amount of time. If not, then execution time should increase. Add the timing code as described below in the comments. You are welcome to extend the code to collect results and process them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOArxQQ0zX6Q"
      },
      "source": [
        "import random,time,multiprocessing\n",
        "\n",
        "def workThatCPU(numLoops):\n",
        "    id = multiprocessing.current_process()\n",
        "    # Print that this process is executing at this time\n",
        "    # YOUR CODE HERE\n",
        "    # Insert code that captures the current time in milliseconds\n",
        "    # YOUR CODE HERE\n",
        "    \n",
        "    for w in range(numLoops):\n",
        "        random.random()\n",
        "    # Print the execution time of the process\n",
        "    #YOUR CODE HERE\n",
        "\n",
        "def coreInvestigation(numCores, numLoops):\n",
        "    processes = []\n",
        "    for w in range(numCores):\n",
        "        p = multiprocessing.Process(target=workThatCPU,args=(numLoops,))\n",
        "        processes.append(p)\n",
        "        p.start()\n",
        "    for p in processes:\n",
        "        p.join()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onImCx5czX6T"
      },
      "source": [
        "Each of our processes will be mapped to a physical core.  Starting from 1 process, the time taken for each process should remain constant till we exceed the available number of cores, typically 4 cores on modern desktop machines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpbPdl3QzX6U"
      },
      "source": [
        "coreInvestigation(1,100000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGe9hps-zX6X"
      },
      "source": [
        "## A multiprocessor Web Crawler\n",
        "We're going to build a web crawler to answer the question - how many neighbours of neighbours does a given web site have? A web crawler retrieves the HTML corresponding to a URL from a web server, parses the HTML to extract the data that is interesting, and then processes the data. This is a typical application for multiprocessing, since the retrieval of the html corresponding to a url may take an arbitrary amount of time, depending upon the bandwidth of the link to the web server, and the load upon the server.\n",
        "The preferred parser is the Beautiful_Soup package, and the connection to the web server is managed by the requests package. Our worker is going to collect a URL from a queue, retrieve and parse the HTML for references to other URLs, and add the set of discovered locations to the queue. Note that these are locations, so \"http\" needs to be prefixed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz_gnmBlzX6Y"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "import multiprocessing\n",
        "\n",
        "def getLocations(url,completed):  \n",
        "    page  = requests.get(url)\n",
        "    data = page.text\n",
        "    base = urlparse(url)\n",
        "    completed.add(base.netloc)\n",
        "    # BS likes to know which parser to use\n",
        "    soup = BeautifulSoup(data, \"html.parser\")\n",
        "    urls = set()\n",
        "    for link in soup.find_all('a'):\n",
        "        if(link.get('href')):\n",
        "            r = urlparse(link.get('href'))\n",
        "            if r.scheme and \"http\" in r.scheme and r.netloc not in completed:\n",
        "                urls.add(r.netloc)\n",
        "    return urls\n",
        "\n",
        "def worker(urlqueue,baseloc):\n",
        "    # create a completed set with the baseloc\n",
        "    completed = set((baseloc))\n",
        "    #baseUrl =  'http://' + baseloc + '/'\n",
        "    results = set()\n",
        "    while not urlqueue.empty():\n",
        "        # TODO pull the first location from the queue\n",
        "        # YOUR CODE HERE\n",
        "        # TODO Create the url from the location\n",
        "        # YOUR CODE HERE'\n",
        "        # TODO get the neighbour locations\n",
        "        # YOUR CODE HERE\n",
        "        results.update(locations)\n",
        "        completed.add(startLoc)\n",
        "    return results\n",
        "    \n",
        "def crawler(url):\n",
        "    manager = multiprocessing.Manager()\n",
        "    myQueue = manager.Queue()\n",
        "    startLoc = urlparse(url)\n",
        "    startSet = getLocations(url,set())\n",
        "    for u in startSet:\n",
        "        myQueue.put(u)\n",
        "    pool = multiprocessing.Pool(5)\n",
        "    results = pool.apply_async(worker,(myQueue,startLoc.netloc))\n",
        "    # Return asynchronous results, that will wait til we have everything completed\n",
        "    return results.get()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e96EVivpzX6b"
      },
      "source": [
        "crawler(\"http://www.sussex.ac.uk\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWwzM8gZzX6f"
      },
      "source": [
        "### Webcrawler problems\n",
        "We can pass any URL to our webcrawler, and it will use the number of processes in the pool to get the results.  However, even though our Queue is synchronized, there are still potential problems.  We want to exit the process when there are no longer any locations to examine.  However, in between seeing if the queue is empty, and getting the next location, the queue may be accessed by another process.  We could fix this by making the check and retrieval a critical section with mutual exclusion, but its better in this situation to just accept that the queue may have emptied by the time we come to look at it. In general, always try to construct solutions that don't need synchronization, except that which is already built into the data structures. "
      ]
    }
  ]
}